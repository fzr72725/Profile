{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('data/train_MNIST.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_tr = data_train.drop('label',axis=1)\n",
    "y_tr = data_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to scale features in order for the softmax to learn under certain set param values, here the scaling range is 0~1\n",
    "scaler = MinMaxScaler(copy=True)\n",
    "scaler.fit(X_tr)\n",
    "X_tr_scaled = scaler.transform(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,[None,784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initail weight, real value to be learned\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "#initial bias, real value to be learned\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x,W)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform y_tr from numerical labels to one_hot code format\n",
    "y_tr[:5]\n",
    "#y_tr_code = pd.get_dummies(y_tr)\n",
    "y_tr_code = np.array(pd.get_dummies(y_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_tr_scaled[:32000]\n",
    "X_test = X_tr_scaled[32000:]\n",
    "y_train = y_tr_code[:32000]\n",
    "y_test = y_tr_code[32000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define cross_entropy\n",
    "y_ = tf.placeholder(tf.float32,[None,10])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y),reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.01386\n"
     ]
    }
   ],
   "source": [
    "#_, loss_val = sess.run([train_step,cross_entropy], feed_dict={x: X_tr_scaled[0:100], y_: y_tr_code[0:100]})\n",
    "#_, loss_val = sess.run([train_step,cross_entropy], feed_dict={x: X_tr_scaled[100:200], y_: y_tr_code[100:200]})\n",
    "#_, loss_val = sess.run([train_step,cross_entropy], feed_dict={x: X_tr[200:300], y_: y_tr_code[200:300]})\n",
    "#_, loss_val = sess.run([train_step,cross_entropy], feed_dict={x: X_tr[300:400], y_: y_tr_code[300:400]})\n",
    "#_, loss_val = sess.run([train_step,cross_entropy], feed_dict={x: X_tr[400:500], y_: y_tr_code[400:500]})\n",
    "#print loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    batch_xs = X_tr_scaled[i*100:(i+1)*100]\n",
    "    batch_ys = y_tr_code[i*100:(i+1)*100]\n",
    "    _, loss_val = sess.run([train_step,cross_entropy], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    #print loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for i in range(2):\n",
    "    #batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    #_, loss_val = sess.run([train_step,cross_entropy], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    ##print loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91031\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(accuracy, feed_dict={x: X_tr_scaled, y_: y_tr_code}))\n",
    "\n",
    "#print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**91% is pretty low, especially for neural network(a simple knn model can get 97%!)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##To get a more respectable result, trying convolutional NN##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First convolutional layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second convolutional layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fully connected layer\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add dropout to prevent overfitting\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Readout layer (softmax)\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = tf.argmax(y_conv,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs_completed = 0\n",
    "index_in_epoch = 0\n",
    "num_examples = X_train.shape[0]\n",
    "train_images = X_train\n",
    "train_labels = y_train\n",
    "\n",
    "# Introduce next_batch function to serve data by batches on a rolling basis\n",
    "def next_batch(batch_size):\n",
    "    \n",
    "    global train_images\n",
    "    global train_labels\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # when all trainig data have been already used, it is reorder randomly    \n",
    "    if index_in_epoch > num_examples:\n",
    "        # finished epoch\n",
    "        epochs_completed += 1\n",
    "        # shuffle the data\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        train_images = X_train[perm]\n",
    "        train_labels = y_train[perm]\n",
    "        # start next epoch\n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "        assert batch_size <= num_examples\n",
    "    end = index_in_epoch\n",
    "    return train_images[start:end], train_labels[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.06\n",
      "step 200, training accuracy 0.94\n",
      "step 400, training accuracy 0.94\n",
      "step 600, training accuracy 0.94\n",
      "step 800, training accuracy 0.9\n",
      "step 1000, training accuracy 0.94\n",
      "step 1200, training accuracy 1\n",
      "step 1400, training accuracy 0.94\n",
      "step 1600, training accuracy 0.94\n",
      "step 1800, training accuracy 0.96\n",
      "step 2000, training accuracy 1\n",
      "step 2200, training accuracy 1\n",
      "step 2400, training accuracy 1\n",
      "step 2600, training accuracy 0.96\n",
      "step 2800, training accuracy 0.98\n",
      "step 3000, training accuracy 1\n",
      "step 3200, training accuracy 1\n",
      "step 3400, training accuracy 0.94\n",
      "step 3600, training accuracy 1\n",
      "step 3800, training accuracy 0.98\n",
      "step 4000, training accuracy 1\n",
      "step 4200, training accuracy 1\n",
      "step 4400, training accuracy 1\n",
      "step 4600, training accuracy 1\n",
      "step 4800, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5400, training accuracy 0.98\n",
      "step 5600, training accuracy 1\n",
      "step 5800, training accuracy 1\n",
      "step 6000, training accuracy 0.96\n",
      "step 6200, training accuracy 1\n",
      "step 6400, training accuracy 0.98\n",
      "step 6600, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8800, training accuracy 0.98\n",
      "step 9000, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9800, training accuracy 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(20000):\n",
    "    batch_size = 50\n",
    "    batch_xs, batch_ys = next_batch(batch_size) \n",
    "    #batch_xs = X_tr_scaled[i*batch_size:(i+1)*batch_size]\n",
    "    #batch_ys = y_tr_code[i*batch_size:(i+1)*batch_size]\n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: batch_xs, y_: batch_ys, keep_prob:1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        #print predict.eval(feed_dict={x: batch_xs[1 : 10], keep_prob: 1.0})\n",
    "    train_step.run(feed_dict={x: batch_xs, y_: batch_ys, keep_prob:0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.9905\n"
     ]
    }
   ],
   "source": [
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "    x: X_test, y_: y_test, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Load testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_te = pd.read_csv('data/test_MNIST.csv')\n",
    "X_te['ImageId'] = X_te.index.values+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(X_te.ImageId, columns=['ImageId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_te.drop('ImageId',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(copy=True)\n",
    "scaler.fit(X_te)\n",
    "X_te_scaled = scaler.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_lables = np.zeros(X_te_scaled.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,X_te_scaled.shape[0]//50):\n",
    "    predicted_lables[i*50 : (i+1)*50] = predict.eval(feed_dict={x: X_te_scaled[i*50 : (i+1)*50], keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  0.,  9.,  9.,  3.,  7.,  0.,  3.,  0.,  3.])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_lables[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  0.,  9.,  9.,  3.,  7.,  0.,  3.,  0.,  3.])"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_lables[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit['Label'] = predicted_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.Label = submit.Label.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId  Label\n",
       "0        1      2\n",
       "1        2      0\n",
       "2        3      9\n",
       "3        4      9\n",
       "4        5      3"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit.to_csv('submisson_tensor_CNN.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
